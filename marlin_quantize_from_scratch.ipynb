{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 32/32 [00:00<00:00, 463.65 examples/s]\n"]}],"source":["MAX_SEQ_LEN = 512\n","\n","model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","ds = dataset.shuffle().select(range(32))\n","\n","def preprocess(example):\n","    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n","\n","ds = ds.map(preprocess)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n","\n","examples = [\n","    tokenizer(\n","        example[\"text\"], padding=False, max_length=MAX_SEQ_LEN, truncation=True,\n","    ) for example in ds\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["quantize_config = BaseQuantizeConfig(\n","    bits=4,                         # quantize model to 4-bit\n","    group_size=128,                 # it is recommended to set the value to 128\n","    desc_act=False,                 # set to False can significantly speed up inference but the perplexity may slightly bad\n","    model_file_base_name=\"model\"    # name of the object when we call save_pretrained\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config, device_map=\"auto\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO - Start quantizing layer 1/22\n","INFO - Quantizing self_attn.k_proj in layer 1/22...\n","INFO - Quantizing self_attn.v_proj in layer 1/22...\n","INFO - Quantizing self_attn.q_proj in layer 1/22...\n","INFO - Quantizing self_attn.o_proj in layer 1/22...\n","INFO - Quantizing mlp.up_proj in layer 1/22...\n","INFO - Quantizing mlp.gate_proj in layer 1/22...\n","INFO - Quantizing mlp.down_proj in layer 1/22...\n","INFO - Start quantizing layer 2/22\n","INFO - Quantizing self_attn.k_proj in layer 2/22...\n","INFO - Quantizing self_attn.v_proj in layer 2/22...\n","INFO - Quantizing self_attn.q_proj in layer 2/22...\n","INFO - Quantizing self_attn.o_proj in layer 2/22...\n","INFO - Quantizing mlp.up_proj in layer 2/22...\n","INFO - Quantizing mlp.gate_proj in layer 2/22...\n","INFO - Quantizing mlp.down_proj in layer 2/22...\n","INFO - Start quantizing layer 3/22\n","INFO - Quantizing self_attn.k_proj in layer 3/22...\n","INFO - Quantizing self_attn.v_proj in layer 3/22...\n","INFO - Quantizing self_attn.q_proj in layer 3/22...\n","INFO - Quantizing self_attn.o_proj in layer 3/22...\n","INFO - Quantizing mlp.up_proj in layer 3/22...\n","INFO - Quantizing mlp.gate_proj in layer 3/22...\n","INFO - Quantizing mlp.down_proj in layer 3/22...\n","INFO - Start quantizing layer 4/22\n","INFO - Quantizing self_attn.k_proj in layer 4/22...\n","INFO - Quantizing self_attn.v_proj in layer 4/22...\n","INFO - Quantizing self_attn.q_proj in layer 4/22...\n","INFO - Quantizing self_attn.o_proj in layer 4/22...\n","INFO - Quantizing mlp.up_proj in layer 4/22...\n","INFO - Quantizing mlp.gate_proj in layer 4/22...\n","INFO - Quantizing mlp.down_proj in layer 4/22...\n","INFO - Start quantizing layer 5/22\n","INFO - Quantizing self_attn.k_proj in layer 5/22...\n","INFO - Quantizing self_attn.v_proj in layer 5/22...\n","INFO - Quantizing self_attn.q_proj in layer 5/22...\n","INFO - Quantizing self_attn.o_proj in layer 5/22...\n","INFO - Quantizing mlp.up_proj in layer 5/22...\n","INFO - Quantizing mlp.gate_proj in layer 5/22...\n","INFO - Quantizing mlp.down_proj in layer 5/22...\n","INFO - Start quantizing layer 6/22\n","INFO - Quantizing self_attn.k_proj in layer 6/22...\n","INFO - Quantizing self_attn.v_proj in layer 6/22...\n","INFO - Quantizing self_attn.q_proj in layer 6/22...\n","INFO - Quantizing self_attn.o_proj in layer 6/22...\n","INFO - Quantizing mlp.up_proj in layer 6/22...\n","INFO - Quantizing mlp.gate_proj in layer 6/22...\n","INFO - Quantizing mlp.down_proj in layer 6/22...\n","INFO - Start quantizing layer 7/22\n","INFO - Quantizing self_attn.k_proj in layer 7/22...\n","INFO - Quantizing self_attn.v_proj in layer 7/22...\n","INFO - Quantizing self_attn.q_proj in layer 7/22...\n","INFO - Quantizing self_attn.o_proj in layer 7/22...\n","INFO - Quantizing mlp.up_proj in layer 7/22...\n","INFO - Quantizing mlp.gate_proj in layer 7/22...\n","INFO - Quantizing mlp.down_proj in layer 7/22...\n","INFO - Start quantizing layer 8/22\n","INFO - Quantizing self_attn.k_proj in layer 8/22...\n","INFO - Quantizing self_attn.v_proj in layer 8/22...\n","INFO - Quantizing self_attn.q_proj in layer 8/22...\n","INFO - Quantizing self_attn.o_proj in layer 8/22...\n","INFO - Quantizing mlp.up_proj in layer 8/22...\n","INFO - Quantizing mlp.gate_proj in layer 8/22...\n","INFO - Quantizing mlp.down_proj in layer 8/22...\n","INFO - Start quantizing layer 9/22\n","INFO - Quantizing self_attn.k_proj in layer 9/22...\n","INFO - Quantizing self_attn.v_proj in layer 9/22...\n","INFO - Quantizing self_attn.q_proj in layer 9/22...\n","INFO - Quantizing self_attn.o_proj in layer 9/22...\n","INFO - Quantizing mlp.up_proj in layer 9/22...\n","INFO - Quantizing mlp.gate_proj in layer 9/22...\n","INFO - Quantizing mlp.down_proj in layer 9/22...\n","INFO - Start quantizing layer 10/22\n","INFO - Quantizing self_attn.k_proj in layer 10/22...\n","INFO - Quantizing self_attn.v_proj in layer 10/22...\n","INFO - Quantizing self_attn.q_proj in layer 10/22...\n","INFO - Quantizing self_attn.o_proj in layer 10/22...\n","INFO - Quantizing mlp.up_proj in layer 10/22...\n","INFO - Quantizing mlp.gate_proj in layer 10/22...\n","INFO - Quantizing mlp.down_proj in layer 10/22...\n","INFO - Start quantizing layer 11/22\n","INFO - Quantizing self_attn.k_proj in layer 11/22...\n","INFO - Quantizing self_attn.v_proj in layer 11/22...\n","INFO - Quantizing self_attn.q_proj in layer 11/22...\n","INFO - Quantizing self_attn.o_proj in layer 11/22...\n","INFO - Quantizing mlp.up_proj in layer 11/22...\n","INFO - Quantizing mlp.gate_proj in layer 11/22...\n","INFO - Quantizing mlp.down_proj in layer 11/22...\n","INFO - Start quantizing layer 12/22\n","INFO - Quantizing self_attn.k_proj in layer 12/22...\n","INFO - Quantizing self_attn.v_proj in layer 12/22...\n","INFO - Quantizing self_attn.q_proj in layer 12/22...\n","INFO - Quantizing self_attn.o_proj in layer 12/22...\n","INFO - Quantizing mlp.up_proj in layer 12/22...\n","INFO - Quantizing mlp.gate_proj in layer 12/22...\n","INFO - Quantizing mlp.down_proj in layer 12/22...\n","INFO - Start quantizing layer 13/22\n","INFO - Quantizing self_attn.k_proj in layer 13/22...\n","INFO - Quantizing self_attn.v_proj in layer 13/22...\n","INFO - Quantizing self_attn.q_proj in layer 13/22...\n","INFO - Quantizing self_attn.o_proj in layer 13/22...\n","INFO - Quantizing mlp.up_proj in layer 13/22...\n","INFO - Quantizing mlp.gate_proj in layer 13/22...\n","INFO - Quantizing mlp.down_proj in layer 13/22...\n","INFO - Start quantizing layer 14/22\n","INFO - Quantizing self_attn.k_proj in layer 14/22...\n","INFO - Quantizing self_attn.v_proj in layer 14/22...\n","INFO - Quantizing self_attn.q_proj in layer 14/22...\n","INFO - Quantizing self_attn.o_proj in layer 14/22...\n","INFO - Quantizing mlp.up_proj in layer 14/22...\n","INFO - Quantizing mlp.gate_proj in layer 14/22...\n","INFO - Quantizing mlp.down_proj in layer 14/22...\n","INFO - Start quantizing layer 15/22\n","INFO - Quantizing self_attn.k_proj in layer 15/22...\n","INFO - Quantizing self_attn.v_proj in layer 15/22...\n","INFO - Quantizing self_attn.q_proj in layer 15/22...\n","INFO - Quantizing self_attn.o_proj in layer 15/22...\n","INFO - Quantizing mlp.up_proj in layer 15/22...\n","INFO - Quantizing mlp.gate_proj in layer 15/22...\n","INFO - Quantizing mlp.down_proj in layer 15/22...\n","INFO - Start quantizing layer 16/22\n","INFO - Quantizing self_attn.k_proj in layer 16/22...\n","INFO - Quantizing self_attn.v_proj in layer 16/22...\n","INFO - Quantizing self_attn.q_proj in layer 16/22...\n","INFO - Quantizing self_attn.o_proj in layer 16/22...\n","INFO - Quantizing mlp.up_proj in layer 16/22...\n","INFO - Quantizing mlp.gate_proj in layer 16/22...\n","INFO - Quantizing mlp.down_proj in layer 16/22...\n","INFO - Start quantizing layer 17/22\n","INFO - Quantizing self_attn.k_proj in layer 17/22...\n","INFO - Quantizing self_attn.v_proj in layer 17/22...\n","INFO - Quantizing self_attn.q_proj in layer 17/22...\n","INFO - Quantizing self_attn.o_proj in layer 17/22...\n","INFO - Quantizing mlp.up_proj in layer 17/22...\n","INFO - Quantizing mlp.gate_proj in layer 17/22...\n","INFO - Quantizing mlp.down_proj in layer 17/22...\n","INFO - Start quantizing layer 18/22\n","INFO - Quantizing self_attn.k_proj in layer 18/22...\n","INFO - Quantizing self_attn.v_proj in layer 18/22...\n","INFO - Quantizing self_attn.q_proj in layer 18/22...\n","INFO - Quantizing self_attn.o_proj in layer 18/22...\n","INFO - Quantizing mlp.up_proj in layer 18/22...\n","INFO - Quantizing mlp.gate_proj in layer 18/22...\n","INFO - Quantizing mlp.down_proj in layer 18/22...\n","INFO - Start quantizing layer 19/22\n","INFO - Quantizing self_attn.k_proj in layer 19/22...\n","INFO - Quantizing self_attn.v_proj in layer 19/22...\n","INFO - Quantizing self_attn.q_proj in layer 19/22...\n","INFO - Quantizing self_attn.o_proj in layer 19/22...\n","INFO - Quantizing mlp.up_proj in layer 19/22...\n","INFO - Quantizing mlp.gate_proj in layer 19/22...\n","INFO - Quantizing mlp.down_proj in layer 19/22...\n","INFO - Start quantizing layer 20/22\n","INFO - Quantizing self_attn.k_proj in layer 20/22...\n","INFO - Quantizing self_attn.v_proj in layer 20/22...\n","INFO - Quantizing self_attn.q_proj in layer 20/22...\n","INFO - Quantizing self_attn.o_proj in layer 20/22...\n","INFO - Quantizing mlp.up_proj in layer 20/22...\n","INFO - Quantizing mlp.gate_proj in layer 20/22...\n","INFO - Quantizing mlp.down_proj in layer 20/22...\n","INFO - Start quantizing layer 21/22\n","INFO - Quantizing self_attn.k_proj in layer 21/22...\n","INFO - Quantizing self_attn.v_proj in layer 21/22...\n","INFO - Quantizing self_attn.q_proj in layer 21/22...\n","INFO - Quantizing self_attn.o_proj in layer 21/22...\n","INFO - Quantizing mlp.up_proj in layer 21/22...\n","INFO - Quantizing mlp.gate_proj in layer 21/22...\n","INFO - Quantizing mlp.down_proj in layer 21/22...\n","INFO - Start quantizing layer 22/22\n","INFO - Quantizing self_attn.k_proj in layer 22/22...\n","INFO - Quantizing self_attn.v_proj in layer 22/22...\n","INFO - Quantizing self_attn.q_proj in layer 22/22...\n","INFO - Quantizing self_attn.o_proj in layer 22/22...\n","INFO - Quantizing mlp.up_proj in layer 22/22...\n","INFO - Quantizing mlp.gate_proj in layer 22/22...\n","INFO - Quantizing mlp.down_proj in layer 22/22...\n"]}],"source":["model.quantize(examples)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - you are using save_pretrained, which will re-direct to save_quantized.\n"]},{"data":{"text/plain":["('./tinyllama-gptq/tokenizer_config.json',\n"," './tinyllama-gptq/special_tokens_map.json',\n"," './tinyllama-gptq/tokenizer.model',\n"," './tinyllama-gptq/added_tokens.json',\n"," './tinyllama-gptq/tokenizer.json')"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["save_dir = \"./tinyllama-gptq\"\n","model.save_pretrained(save_dir)\n","tokenizer.save_pretrained(save_dir)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n","INFO - The layer lm_head is not quantized.\n","Repacking weights to be compatible with Marlin kernel...: 100%|██████████| 314/314 [01:24<00:00,  3.71it/s]\n","INFO - Disabling fused attention and mlp injection because Marlin kernel is used\n","The safetensors archive passed at ./tinyllama-gptq/autogptq_model.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"]}],"source":["marlin_model = AutoGPTQForCausalLM.from_quantized(save_dir, use_marlin=True, device_map=\"auto\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["--- marlin:\n","In a galaxy far, far away, the Jedi are a peaceful order of beings who live in harmony with the Force. However, in the face of a powerful and malevolent force, the Jedi are forced to fight for their survival. In this game\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","text = \"In a galaxy far, far away\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n","\n","print(\"--- marlin:\")\n","out = marlin_model.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - you are using save_pretrained, which will re-direct to save_quantized.\n"]},{"data":{"text/plain":["('./tinyllama-marlin/tokenizer_config.json',\n"," './tinyllama-marlin/special_tokens_map.json',\n"," './tinyllama-marlin/tokenizer.model',\n"," './tinyllama-marlin/added_tokens.json',\n"," './tinyllama-marlin/tokenizer.json')"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["marlin_model.save_pretrained(\"./tinyllama-marlin\")\n","tokenizer.save_pretrained(\"./tinyllama-marlin\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n","INFO - The layer lm_head is not quantized.\n","Overriding QuantLinear layers to use Marlin's QuantLinear...: 100%|██████████| 314/314 [00:28<00:00, 11.17it/s]\n","INFO - Disabling fused attention and mlp injection because Marlin kernel is used\n","The safetensors archive passed at ./tinyllama-gptq/autogptq_model.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n","WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n","INFO - The layer lm_head is not quantized.\n","Overriding QuantLinear layers to use Marlin's QuantLinear...: 100%|██████████| 314/314 [00:26<00:00, 12.07it/s]\n","INFO - Disabling fused attention and mlp injection because Marlin kernel is used\n"]}],"source":["marlin_model_reloaded_cache = AutoGPTQForCausalLM.from_quantized(\"./tinyllama-gptq\", use_marlin=True, device_map=\"auto\")\n","marlin_model_reloaded_serialized = AutoGPTQForCausalLM.from_quantized(\"./tinyllama-marlin\", use_marlin=True, device_map=\"auto\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["--- marlin cached:\n"]},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["In a galaxy far, far away, the Jedi are a peaceful order of beings who live in harmony with the Force. However, in the face of a powerful and malevolent force, the Jedi are forced to fight for their survival. In this game\n","--- marlin serialized:\n","In a galaxy far, far away, the Jedi are a peaceful order of beings who live in harmony with the Force. However, in the face of a powerful and malevolent force, the Jedi are forced to fight for their survival. In this game\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","text = \"In a galaxy far, far away\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n","\n","print(\"--- marlin cached:\")\n","out = marlin_model_reloaded_cache.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))\n","\n","print(\"--- marlin serialized:\")\n","out = marlin_model_reloaded_serialized.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
