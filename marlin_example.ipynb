{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 128/128 [00:00<00:00, 1166.95 examples/s]\n"]}],"source":["MAX_SEQ_LEN = 2048\n","\n","model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","ds = dataset.shuffle().select(range(128))\n","\n","def preprocess(example):\n","    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n","\n","ds = ds.map(preprocess)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n","\n","examples = [\n","    tokenizer(\n","        example[\"text\"], padding=False, max_length=MAX_SEQ_LEN, truncation=True,\n","    ) for example in ds\n","]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["quantize_config = BaseQuantizeConfig(\n","    bits=4,                         # quantize model to 4-bit\n","    group_size=128,                 # it is recommended to set the value to 128\n","    desc_act=False,                 # set to False can significantly speed up inference but the perplexity may slightly bad\n","    model_file_base_name=\"model\"    # name of the object when we call save_pretrained\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config, device_map=\"auto\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO - Start quantizing layer 1/22\n","INFO - Quantizing self_attn.k_proj in layer 1/22...\n","INFO - Quantizing self_attn.v_proj in layer 1/22...\n","INFO - Quantizing self_attn.q_proj in layer 1/22...\n","INFO - Quantizing self_attn.o_proj in layer 1/22...\n","INFO - Quantizing mlp.up_proj in layer 1/22...\n","INFO - Quantizing mlp.gate_proj in layer 1/22...\n","INFO - Quantizing mlp.down_proj in layer 1/22...\n","INFO - Start quantizing layer 2/22\n","INFO - Quantizing self_attn.k_proj in layer 2/22...\n","INFO - Quantizing self_attn.v_proj in layer 2/22...\n","INFO - Quantizing self_attn.q_proj in layer 2/22...\n","INFO - Quantizing self_attn.o_proj in layer 2/22...\n","INFO - Quantizing mlp.up_proj in layer 2/22...\n","INFO - Quantizing mlp.gate_proj in layer 2/22...\n","INFO - Quantizing mlp.down_proj in layer 2/22...\n","INFO - Start quantizing layer 3/22\n","INFO - Quantizing self_attn.k_proj in layer 3/22...\n","INFO - Quantizing self_attn.v_proj in layer 3/22...\n","INFO - Quantizing self_attn.q_proj in layer 3/22...\n","INFO - Quantizing self_attn.o_proj in layer 3/22...\n","INFO - Quantizing mlp.up_proj in layer 3/22...\n","INFO - Quantizing mlp.gate_proj in layer 3/22...\n","INFO - Quantizing mlp.down_proj in layer 3/22...\n","INFO - Start quantizing layer 4/22\n","INFO - Quantizing self_attn.k_proj in layer 4/22...\n","INFO - Quantizing self_attn.v_proj in layer 4/22...\n","INFO - Quantizing self_attn.q_proj in layer 4/22...\n","INFO - Quantizing self_attn.o_proj in layer 4/22...\n","INFO - Quantizing mlp.up_proj in layer 4/22...\n","INFO - Quantizing mlp.gate_proj in layer 4/22...\n","INFO - Quantizing mlp.down_proj in layer 4/22...\n","INFO - Start quantizing layer 5/22\n","INFO - Quantizing self_attn.k_proj in layer 5/22...\n","INFO - Quantizing self_attn.v_proj in layer 5/22...\n","INFO - Quantizing self_attn.q_proj in layer 5/22...\n","INFO - Quantizing self_attn.o_proj in layer 5/22...\n","INFO - Quantizing mlp.up_proj in layer 5/22...\n","INFO - Quantizing mlp.gate_proj in layer 5/22...\n","INFO - Quantizing mlp.down_proj in layer 5/22...\n","INFO - Start quantizing layer 6/22\n","INFO - Quantizing self_attn.k_proj in layer 6/22...\n","INFO - Quantizing self_attn.v_proj in layer 6/22...\n","INFO - Quantizing self_attn.q_proj in layer 6/22...\n","INFO - Quantizing self_attn.o_proj in layer 6/22...\n","INFO - Quantizing mlp.up_proj in layer 6/22...\n","INFO - Quantizing mlp.gate_proj in layer 6/22...\n","INFO - Quantizing mlp.down_proj in layer 6/22...\n","INFO - Start quantizing layer 7/22\n","INFO - Quantizing self_attn.k_proj in layer 7/22...\n","INFO - Quantizing self_attn.v_proj in layer 7/22...\n","INFO - Quantizing self_attn.q_proj in layer 7/22...\n","INFO - Quantizing self_attn.o_proj in layer 7/22...\n","INFO - Quantizing mlp.up_proj in layer 7/22...\n","INFO - Quantizing mlp.gate_proj in layer 7/22...\n","INFO - Quantizing mlp.down_proj in layer 7/22...\n","INFO - Start quantizing layer 8/22\n","INFO - Quantizing self_attn.k_proj in layer 8/22...\n","INFO - Quantizing self_attn.v_proj in layer 8/22...\n","INFO - Quantizing self_attn.q_proj in layer 8/22...\n","INFO - Quantizing self_attn.o_proj in layer 8/22...\n","INFO - Quantizing mlp.up_proj in layer 8/22...\n","INFO - Quantizing mlp.gate_proj in layer 8/22...\n","INFO - Quantizing mlp.down_proj in layer 8/22...\n","INFO - Start quantizing layer 9/22\n","INFO - Quantizing self_attn.k_proj in layer 9/22...\n","INFO - Quantizing self_attn.v_proj in layer 9/22...\n","INFO - Quantizing self_attn.q_proj in layer 9/22...\n","INFO - Quantizing self_attn.o_proj in layer 9/22...\n","INFO - Quantizing mlp.up_proj in layer 9/22...\n","INFO - Quantizing mlp.gate_proj in layer 9/22...\n","INFO - Quantizing mlp.down_proj in layer 9/22...\n","INFO - Start quantizing layer 10/22\n","INFO - Quantizing self_attn.k_proj in layer 10/22...\n","INFO - Quantizing self_attn.v_proj in layer 10/22...\n","INFO - Quantizing self_attn.q_proj in layer 10/22...\n","INFO - Quantizing self_attn.o_proj in layer 10/22...\n","INFO - Quantizing mlp.up_proj in layer 10/22...\n","INFO - Quantizing mlp.gate_proj in layer 10/22...\n","INFO - Quantizing mlp.down_proj in layer 10/22...\n","INFO - Start quantizing layer 11/22\n","INFO - Quantizing self_attn.k_proj in layer 11/22...\n","INFO - Quantizing self_attn.v_proj in layer 11/22...\n","INFO - Quantizing self_attn.q_proj in layer 11/22...\n","INFO - Quantizing self_attn.o_proj in layer 11/22...\n","INFO - Quantizing mlp.up_proj in layer 11/22...\n","INFO - Quantizing mlp.gate_proj in layer 11/22...\n","INFO - Quantizing mlp.down_proj in layer 11/22...\n","INFO - Start quantizing layer 12/22\n","INFO - Quantizing self_attn.k_proj in layer 12/22...\n","INFO - Quantizing self_attn.v_proj in layer 12/22...\n","INFO - Quantizing self_attn.q_proj in layer 12/22...\n","INFO - Quantizing self_attn.o_proj in layer 12/22...\n","INFO - Quantizing mlp.up_proj in layer 12/22...\n","INFO - Quantizing mlp.gate_proj in layer 12/22...\n","INFO - Quantizing mlp.down_proj in layer 12/22...\n","INFO - Start quantizing layer 13/22\n","INFO - Quantizing self_attn.k_proj in layer 13/22...\n","INFO - Quantizing self_attn.v_proj in layer 13/22...\n","INFO - Quantizing self_attn.q_proj in layer 13/22...\n","INFO - Quantizing self_attn.o_proj in layer 13/22...\n","INFO - Quantizing mlp.up_proj in layer 13/22...\n","INFO - Quantizing mlp.gate_proj in layer 13/22...\n","INFO - Quantizing mlp.down_proj in layer 13/22...\n","INFO - Start quantizing layer 14/22\n","INFO - Quantizing self_attn.k_proj in layer 14/22...\n","INFO - Quantizing self_attn.v_proj in layer 14/22...\n","INFO - Quantizing self_attn.q_proj in layer 14/22...\n","INFO - Quantizing self_attn.o_proj in layer 14/22...\n","INFO - Quantizing mlp.up_proj in layer 14/22...\n","INFO - Quantizing mlp.gate_proj in layer 14/22...\n","INFO - Quantizing mlp.down_proj in layer 14/22...\n","INFO - Start quantizing layer 15/22\n","INFO - Quantizing self_attn.k_proj in layer 15/22...\n","INFO - Quantizing self_attn.v_proj in layer 15/22...\n","INFO - Quantizing self_attn.q_proj in layer 15/22...\n","INFO - Quantizing self_attn.o_proj in layer 15/22...\n","INFO - Quantizing mlp.up_proj in layer 15/22...\n","INFO - Quantizing mlp.gate_proj in layer 15/22...\n","INFO - Quantizing mlp.down_proj in layer 15/22...\n","INFO - Start quantizing layer 16/22\n","INFO - Quantizing self_attn.k_proj in layer 16/22...\n","INFO - Quantizing self_attn.v_proj in layer 16/22...\n","INFO - Quantizing self_attn.q_proj in layer 16/22...\n","INFO - Quantizing self_attn.o_proj in layer 16/22...\n","INFO - Quantizing mlp.up_proj in layer 16/22...\n","INFO - Quantizing mlp.gate_proj in layer 16/22...\n","INFO - Quantizing mlp.down_proj in layer 16/22...\n","INFO - Start quantizing layer 17/22\n","INFO - Quantizing self_attn.k_proj in layer 17/22...\n","INFO - Quantizing self_attn.v_proj in layer 17/22...\n","INFO - Quantizing self_attn.q_proj in layer 17/22...\n","INFO - Quantizing self_attn.o_proj in layer 17/22...\n","INFO - Quantizing mlp.up_proj in layer 17/22...\n","INFO - Quantizing mlp.gate_proj in layer 17/22...\n","INFO - Quantizing mlp.down_proj in layer 17/22...\n","INFO - Start quantizing layer 18/22\n","INFO - Quantizing self_attn.k_proj in layer 18/22...\n","INFO - Quantizing self_attn.v_proj in layer 18/22...\n","INFO - Quantizing self_attn.q_proj in layer 18/22...\n","INFO - Quantizing self_attn.o_proj in layer 18/22...\n","INFO - Quantizing mlp.up_proj in layer 18/22...\n","INFO - Quantizing mlp.gate_proj in layer 18/22...\n","INFO - Quantizing mlp.down_proj in layer 18/22...\n","INFO - Start quantizing layer 19/22\n","INFO - Quantizing self_attn.k_proj in layer 19/22...\n","INFO - Quantizing self_attn.v_proj in layer 19/22...\n","INFO - Quantizing self_attn.q_proj in layer 19/22...\n","INFO - Quantizing self_attn.o_proj in layer 19/22...\n","INFO - Quantizing mlp.up_proj in layer 19/22...\n","INFO - Quantizing mlp.gate_proj in layer 19/22...\n","INFO - Quantizing mlp.down_proj in layer 19/22...\n","INFO - Start quantizing layer 20/22\n","INFO - Quantizing self_attn.k_proj in layer 20/22...\n","INFO - Quantizing self_attn.v_proj in layer 20/22...\n","INFO - Quantizing self_attn.q_proj in layer 20/22...\n","INFO - Quantizing self_attn.o_proj in layer 20/22...\n","INFO - Quantizing mlp.up_proj in layer 20/22...\n","INFO - Quantizing mlp.gate_proj in layer 20/22...\n","INFO - Quantizing mlp.down_proj in layer 20/22...\n","INFO - Start quantizing layer 21/22\n","INFO - Quantizing self_attn.k_proj in layer 21/22...\n","INFO - Quantizing self_attn.v_proj in layer 21/22...\n","INFO - Quantizing self_attn.q_proj in layer 21/22...\n","INFO - Quantizing self_attn.o_proj in layer 21/22...\n","INFO - Quantizing mlp.up_proj in layer 21/22...\n","INFO - Quantizing mlp.gate_proj in layer 21/22...\n","INFO - Quantizing mlp.down_proj in layer 21/22...\n","INFO - Start quantizing layer 22/22\n","INFO - Quantizing self_attn.k_proj in layer 22/22...\n","INFO - Quantizing self_attn.v_proj in layer 22/22...\n","INFO - Quantizing self_attn.q_proj in layer 22/22...\n","INFO - Quantizing self_attn.o_proj in layer 22/22...\n","INFO - Quantizing mlp.up_proj in layer 22/22...\n","INFO - Quantizing mlp.gate_proj in layer 22/22...\n","INFO - Quantizing mlp.down_proj in layer 22/22...\n"]}],"source":["model.quantize(examples)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["'gptq_model-4bit-128g'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["quantize_config.model_name_or_path\n","quantize_config.model_file_base_name"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - you are using save_pretrained, which will re-direct to save_quantized.\n"]},{"data":{"text/plain":["('./tinyllama-gptq/tokenizer_config.json',\n"," './tinyllama-gptq/special_tokens_map.json',\n"," './tinyllama-gptq/tokenizer.model',\n"," './tinyllama-gptq/added_tokens.json',\n"," './tinyllama-gptq/tokenizer.json')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["save_dir = \"./tinyllama-gptq\"\n","model.save_pretrained(save_dir)\n","tokenizer.save_pretrained(save_dir)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO - The layer lm_head is not quantized.\n"]},{"name":"stderr","output_type":"stream","text":["Repacking weights to be compatible with Marlin kernel...: 100%|██████████| 314/314 [01:16<00:00,  4.10it/s]\n","INFO - Disabling fused attention and mlp injection because Marlin kernel is used\n","The safetensors archive passed at ./tinyllama-gptq/autogptq_model.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"]}],"source":["marlin_model = AutoGPTQForCausalLM.from_quantized(save_dir, use_marlin=True, device_map=\"auto\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO - The layer lm_head is not quantized.\n","Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.\n"]}],"source":["gptq_model = AutoGPTQForCausalLM.from_quantized(save_dir, use_marlin=False, device_map=\"auto\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["--- marlin:\n","In a galaxy far, far away, the Jedi Knights were a group of highly trained and skilled warriors who fought against\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","text = \"In a galaxy far, far away\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n","\n","print(\"--- marlin:\")\n","out = marlin_model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_5XT5NDDIaPW","outputId":"1f062263-d490-4a4e-8c3e-d56537904f11"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n","\n","model_id = \"facebook/opt-125m\"\n","quantization_config = GPTQConfig(\n","     bits=4,\n","     group_size=128,\n","     dataset=\"wikitext2\",\n","     desc_act=False,\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Im2QF34XIaPW","outputId":"27dcf005-d04e-4ed8-d998-7ea26c173e61"},"outputs":[{"name":"stderr","output_type":"stream","text":["Quantizing model.decoder.layers blocks : 100%|██████████| 12/12 [01:49<00:00,  9.17s/it]\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","quant_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map='auto')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eA6G93nPIaPX","outputId":"5963fcda-ea39-44a4-8ae8-ec9e0d4ecaa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["In a galaxy far, far away, there is no way to stop the spread of the virus.\n","\n","The virus is spreading,\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","text = \"In a galaxy far, far away\"\n","inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n","\n","print(\"--- marlin\")\n","out = marlin_model.generate(**inputs, max_new_tokens=20)\n","print(tokenizer.decode(out[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GI4-gZbrIaPY"},"outputs":[],"source":["quant_model.save_pretrained(\"./opt-quantized\")\n","tokenizer.save_pretrained(\"./opt-quantized\")\n","quantization_config.to_json_file(\"./opt-quantized/quantize_config.json\")"]},{"cell_type":"markdown","metadata":{"id":"zTBc5K77IaPY"},"source":["#### Restart The Runtime"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"RKJKJ2CXIaPZ","outputId":"4eb595b5-b91e-46ad-9abc-e6f52c1bebe5"},"outputs":[{"data":{"text/plain":["odict_keys(['qweight', 'qzeros', 'scales', 'g_idx', 'bias'])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoModelForCausalLM\n","model_id = \"./opt-quantized\"\n","quant_model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto')\n","quant_model.model.decoder.layers[0].self_attn.k_proj._buffers.keys()"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING - ignoring unknown parameter in quantize_config.json: batch_size.\n","WARNING - ignoring unknown parameter in quantize_config.json: block_name_to_quantize.\n","WARNING - ignoring unknown parameter in quantize_config.json: cache_block_outputs.\n","WARNING - ignoring unknown parameter in quantize_config.json: dataset.\n","WARNING - ignoring unknown parameter in quantize_config.json: exllama_config.\n","WARNING - ignoring unknown parameter in quantize_config.json: max_input_length.\n","WARNING - ignoring unknown parameter in quantize_config.json: model_seqlen.\n","WARNING - ignoring unknown parameter in quantize_config.json: module_name_preceding_first_block.\n","WARNING - ignoring unknown parameter in quantize_config.json: modules_in_block_to_quantize.\n","WARNING - ignoring unknown parameter in quantize_config.json: pad_token_id.\n","WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n","WARNING - ignoring unknown parameter in quantize_config.json: tokenizer.\n","WARNING - ignoring unknown parameter in quantize_config.json: use_cuda_fp16.\n","WARNING - ignoring unknown parameter in quantize_config.json: use_exllama.\n","INFO - The layer lm_head is not quantized.\n","Repacking weights to be compatible with Marlin kernel...: 100%|██████████| 140/140 [00:18<00:00,  7.52it/s]\n","INFO - Disabling fused attention and mlp injection because Marlin kernel is used\n"]},{"ename":"OSError","evalue":"The safetensors archive passed at ./opt-quantized/autogptq_model.safetensors does not contain the valid metadata. Make sure you save your model with the `save_pretrained` method.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mauto_gptq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoGPTQForCausalLM\n\u001b[1;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./opt-quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m marlin_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoGPTQForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_quantized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_marlin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/AutoGPTQ/auto_gptq/modeling/auto.py:133\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# TODO: do we need this filtering of kwargs? @PanQiWei is there a reason we can't just pass all kwargs?\u001b[39;00m\n\u001b[1;32m    128\u001b[0m keywords \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m     key: kwargs[key]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(signature(quant_func)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m huggingface_kwargs\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    132\u001b[0m }\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquant_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_triton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_triton\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minject_fused_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minject_fused_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43minject_fused_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minject_fused_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cuda_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cuda_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantize_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_basename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_basename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_triton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_triton\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_exllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_exllama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_exllamav2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_exllamav2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/AutoGPTQ/auto_gptq/modeling/_base.py:1117\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, use_qigen, use_marlin, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         inject_fused_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m         inject_fused_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m \u001b[43maccelerate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodeling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint_in_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is very hacky but works due to https://github.com/huggingface/accelerate/blob/bd72a5f1a80d5146554458823f8aeda0a9db5297/src/accelerate/utils/modeling.py#L292\u001b[39;49;00m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# TODO: Why are we using this custom function and not dispatch_model?\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m model \u001b[38;5;241m=\u001b[39m simple_dispatch_model(model, device_map)\n","File \u001b[0;32m~/auto-gptq-env/lib/python3.10/site-packages/accelerate/utils/modeling.py:1493\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)\u001b[0m\n\u001b[1;32m   1491\u001b[0m buffer_names \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_buffers()]\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m checkpoint_file \u001b[38;5;129;01min\u001b[39;00m checkpoint_files:\n\u001b[0;32m-> 1493\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1495\u001b[0m         model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m~/auto-gptq-env/lib/python3.10/site-packages/accelerate/utils/modeling.py:1263\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, device_map)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe safetensors archive passed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not contain the valid metadata. Make sure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou save your model with the `save_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint passed was saved with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, we need a the pt format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mOSError\u001b[0m: The safetensors archive passed at ./opt-quantized/autogptq_model.safetensors does not contain the valid metadata. Make sure you save your model with the `save_pretrained` method."]}],"source":["from auto_gptq import AutoGPTQForCausalLM\n","model_id = \"./opt-quantized\"\n","marlin_model = AutoGPTQForCausalLM.from_quantized(model_id, use_marlin=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
